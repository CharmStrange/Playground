## W7 : 머신 러닝 기법 : 회귀
> 분류 모델은 이산적인 `Label` 에 대해 목표를 잡았지만, 회귀 모델은 연속적인 `Label` 에 대해 목표를 잡는다.
>
> ***회귀 문제에선, 원인과 결과의 인과 관계적 사고를 지양해야 한다. `"관계가 존재한다."` --> True or False ... `"~때문에 ~이다."` --> False***
>
> 기본적으로 선형 회귀 모델을 가정하면, $w$ 라는 모델의 파라미터(가중치)가 등장한다. 선형 회귀 모델이 목표한 `Label` 을 잘 맞출 수 있도록 하기 위해선 모델의 파라미터를 최적화시키는 것이 매우 중요하다.
>
> > ### Least Square
> > 예측 값과 실제 값과 차이, 즉 오차를 최소화하는 선형 모델을 찾기 위해 사용되는 것이 **Least Square** 이다.
> >
> > **Least Square** = $(wx_i-y_i)^2$
> >
> > 데이터셋의 모든 *feature* value 는 이 **Least Square** 를 모두 가지고 있으며 선형 모델은 모델 전체의 **Least Square** 를 최소화하는 방향으로 구성되어야 그것이 좋은 선형 회귀 모델이라고 할 수 있는 것이다.
>
> > ### Bias variable
> > 모든 *feature* value 를 선형 회귀 모델에 그대로 사용하게 되면, 그러니까 선형 회귀 모델의 학습에 기존 데이터셋을 그대로 넣어주게 되면 선형 회귀 모델은 무조건 원점을 지나게 된다. 이는 *feature* value 당 하나의 $w$ 를 고려했기 때문이다.
> >
> > ![image](https://github.com/user-attachments/assets/1eb2e46e-8e59-4aca-ae0e-fe7a1975d679)
> >
> > 그래서 선형 회귀 모델에 항 하나(=절편)를 추가해 주어, *feature* value 가 `0` 일때 `Label` 또한 무조건 `0` 이 되는 것을 방지한다.
> >
> > ```
> > X = [ a ]        X = [ a 1 ]
> >     [ b ]   >>>      [ b 1 ]
> >     [ c ]            [ c 1 ]
> > ```
> > 이러면 추가된 `1` 들이 새로운 절편 항 $w$ 를 담당하게 되어 방금의 문제를 해결할 수 있게 된다.
> 
> 선형 회귀 모델의 가장 큰 단점은, 그 형태가 너무 단순해 실제 문제 해결에 사용하기엔 무리가 있다는 점이다. 또한 **Least Square** 는 *Outlier* *feature* value 에 너무 취약하고 학습에 사용될 데이터셋의 크기가 너무 크면 계산 효율이 급격하게 감소하는 단점 등이 존재한다.

## W8 : 회귀 모델을 사용한 분류
> 가장 단순한 회귀를 통한 분류 모델은, 직선 형태의 모델을 두고 그 안에 *threshold* 를 두어 해당 지점을 기준으로 이진 분류를 진행하는 것이다.
> 
> 이진 분류의 `Label` 을 $1$, $-1$ 로 지정한다고 했을 때, 모델의 출력에서 부호를 보고 `Label` 할당이 가능할 것이다. 이 때 *threshold* 는 출력 $= 0$ 인 지점, 즉 $x$ 절편 지점이다. *threshold* 는 *Dicision boundary* 라고 불리기도 한다.
>
> > ### 회귀 모델 분류에서의 Least Square
> > 결론적으로 회귀 모델 분류에서는 **Least Square** 사용이 불가하다.
> >
> > ```
> > (예측 값 : 0.9 , 실제 값 : 1) ... Least Square = 0.01 
> > (예측 값 : -0.9, 실제 값 : 1) ... Least Square = 3.24
> > ...  
> > ```
> > 조금만 값이 틀어져도 오차는 그 값이 매우 커질 수 있기 때문에 **Least Square** 로 최적화를 하지 않는다.
> 
> > ### 0-1 loss function
> > 실제 값과 예측 값의 부호가 서로 같으면 0을 출력하고, 그렇지 않다면 1을 출력하는  새로운 목적 함수이다.
> >
> > $\lVert\hat{y}-y\rVert$
> >
> > ![image](https://github.com/user-attachments/assets/531a1df5-f172-46f9-9993-2115c5298b63)
> >
> > 보통 선형 회귀 모델에서의 파라미터 최적화는 기울기 값이 $0$이 되는 지점을 찾아가는 과정이다. 즉 파라미터에 대해 미분을 했을 때 그 결과가 $0$이 되는 지점을 찾는 것이다. 그런데 **0-1 loss function** 의 그래프를 보면 모든 지점이 상수로 고정되어 있어 파라미터에 대한 미분을 해도 그 값이 0이 나오기 때문에 사실상 파라미터에 대한 최적화 과정이 전부 무의미해진다.
> 
> > ### Max function
> > **0-1 loss function** 에서 발생한 문제, 그러니까 모든 지점에서의 기울기 값이 0인 문제를 해결하기 위해 새롭게 정의한 목적 함수이다.
> > 
> > $\max (0, -(\hat{y}\times y))$
> >
> > 예측 값과 실제 값의 부호가 다르면 $\max (0, +)=+$ 이고,
> > 부호가 다르면 $\max (0, -)=0$ 이라 이렇게 부호가 다른 경우에 한해 파라미터의 최적화가 가능하다.
> >
