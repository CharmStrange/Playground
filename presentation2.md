## 머신 러닝 기법 : 회귀
> 분류 모델은 이산적인 `Label` 에 대해 목표를 잡았지만, 회귀 모델은 연속적인 `Label` 에 대해 목표를 잡는다.
>
> ***회귀 문제에선, 원인과 결과의 인과 관계적 사고를 지양해야 한다. `"관계가 존재한다."` --> True or False ... `"~때문에 ~이다."` --> False***
>
> 기본적으로 선형 회귀 모델을 가정하면, $w$ 라는 모델의 파라미터(가중치)가 등장한다. 선형 회귀 모델이 목표한 `Label` 을 잘 맞출 수 있도록 하기 위해선 모델의 파라미터를 최적화시키는 것이 매우 중요하다.
>
> > ### Least Square
> > 예측 값과 실제 값과 차이, 즉 오차를 최소화하는 선형 모델을 찾기 위해 사용되는 것이 **Least Square** 이다.
> >
> > **Least Square** = $(wx_i-y_i)^2$
> >
> > 데이터셋의 모든 *feature* value 는 이 **Least Square** 를 모두 가지고 있으며 선형 모델은 모델 전체의 **Least Square** 를 최소화하는 방향으로 구성되어야 그것이 좋은 선형 회귀 모델이라고 할 수 있는 것이다.
>
> > ### Bias variable
> > 모든 *feature* value 를 선형 회귀 모델에 그대로 사용하게 되면, 그러니까 선형 회귀 모델의 학습에 기존 데이터셋을 그대로 넣어주게 되면 선형 회귀 모델은 무조건 원점을 지나게 된다. 이는 *feature* value 당 하나의 $w$ 를 고려했기 때문이다.
> >
> > ![image](https://github.com/user-attachments/assets/1eb2e46e-8e59-4aca-ae0e-fe7a1975d679)
> >
> > 그래서 선형 회귀 모델에 항 하나(=절편)를 추가해 주어, *feature* value 가 `0` 일때 `Label` 또한 무조건 `0` 이 되는 것을 방지한다.
> >
> > ```
> > X = [ a ]        X = [ a 1 ]
> >     [ b ]   >>>      [ b 1 ]
> >     [ c ]            [ c 1 ]
> > ```
> > 이러면 추가된 `1` 들이 새로운 절편 항 $w$ 를 담당하게 되어 방금의 문제를 해결할 수 있게 된다.
> 
> 선형 회귀 모델의 가장 큰 단점은, 그 형태가 너무 단순해 실제 문제 해결에 사용하기엔 무리가 있다는 점이다. 또한 **Least Square** 는 *Outlier* *feature* value 에 너무 취약하고 학습에 사용될 데이터셋의 크기가 너무 크면 계산 효율이 급격하게 감소하는 단점 등이 존재한다.
