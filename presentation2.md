## W7 : 머신 러닝 기법 : 회귀
> 분류 모델은 이산적인 `Label` 에 대해 목표를 잡았지만, 회귀 모델은 연속적인 `Label` 에 대해 목표를 잡는다.
>
> ***회귀 문제에선, 원인과 결과의 인과 관계적 사고를 지양해야 한다. `"관계가 존재한다."` --> True or False ... `"~때문에 ~이다."` --> False***
>
> 기본적으로 선형 회귀 모델을 가정하면, $w$ 라는 모델의 파라미터(가중치)가 등장한다. 선형 회귀 모델이 목표한 `Label` 을 잘 맞출 수 있도록 하기 위해선 모델의 파라미터를 최적화시키는 것이 매우 중요하다.
>
> > ### Least Square
> > 예측 값과 실제 값과 차이, 즉 오차를 최소화하는 선형 모델을 찾기 위해 사용되는 것이 **Least Square** 이다.
> >
> > **Least Square** = $(wx_i-y_i)^2$
> >
> > 데이터셋의 모든 *feature* value 는 이 **Least Square** 를 모두 가지고 있으며 선형 모델은 모델 전체의 **Least Square** 를 최소화하는 방향으로 구성되어야 그것이 좋은 선형 회귀 모델이라고 할 수 있는 것이다.
>
> > ### Bias variable
> > 모든 *feature* value 를 선형 회귀 모델에 그대로 사용하게 되면, 그러니까 선형 회귀 모델의 학습에 기존 데이터셋을 그대로 넣어주게 되면 선형 회귀 모델은 무조건 원점을 지나게 된다. 이는 *feature* value 당 하나의 $w$ 를 고려했기 때문이다.
> >
> > ![image](https://github.com/user-attachments/assets/1eb2e46e-8e59-4aca-ae0e-fe7a1975d679)
> >
> > 그래서 선형 회귀 모델에 항 하나(=절편)를 추가해 주어, *feature* value 가 `0` 일때 `Label` 또한 무조건 `0` 이 되는 것을 방지한다.
> >
> > ```
> > X = [ a ]        X = [ a 1 ]
> >     [ b ]   >>>      [ b 1 ]
> >     [ c ]            [ c 1 ]
> > ```
> > 이러면 추가된 `1` 들이 새로운 절편 항 $w$ 를 담당하게 되어 방금의 문제를 해결할 수 있게 된다.
> 
> 선형 회귀 모델의 가장 큰 단점은, 그 형태가 너무 단순해 실제 문제 해결에 사용하기엔 무리가 있다는 점이다. 또한 **Least Square** 는 *Outlier* *feature* value 에 너무 취약하고 학습에 사용될 데이터셋의 크기가 너무 크면 계산 효율이 급격하게 감소하는 단점 등이 존재한다.

## W8 : 회귀 모델을 사용한 분류
> 가장 단순한 회귀를 통한 분류 모델은, 직선 형태의 모델을 두고 그 안에 *threshold* 를 두어 해당 지점을 기준으로 이진 분류를 진행하는 것이다.
> 
> 이진 분류의 `Label` 을 $1$, $-1$ 로 지정한다고 했을 때, 모델의 출력에서 부호를 보고 `Label` 할당이 가능할 것이다. 이 때 *threshold* 는 출력 $= 0$ 인 지점, 즉 $x$ 절편 지점이다. *threshold* 는 *Dicision boundary* 라고 불리기도 한다.
>
> > ### 회귀 모델 분류에서의 Least Square
> > 결론적으로 회귀 모델 분류에서는 **Least Square** 사용이 불가하다.
> >
> > ```
> > (예측 값 : 0.9 , 실제 값 : 1) ... Least Square = 0.01 
> > (예측 값 : -0.9, 실제 값 : 1) ... Least Square = 3.24
> > ...  
> > ```
> > 조금만 값이 틀어져도 오차는 그 값이 매우 커질 수 있기 때문에 **Least Square** 로 최적화를 하지 않는다.
> 
> > ### 0-1 loss function
> > 실제 값과 예측 값의 부호가 서로 같으면 0을 출력하고, 그렇지 않다면 1을 출력하는  새로운 목적 함수이다.
> >
> > $\lVert\hat{y}-y\rVert$
> >
> > ![image](https://github.com/user-attachments/assets/531a1df5-f172-46f9-9993-2115c5298b63)
> >
> > 보통 선형 회귀 모델에서의 파라미터 최적화는 기울기 값이 $0$이 되는 지점을 찾아가는 과정이다. 즉 파라미터에 대해 미분을 했을 때 그 결과가 $0$이 되는 지점을 찾는 것이다. 그런데 **0-1 loss function** 의 그래프를 보면 모든 지점이 상수로 고정되어 있어 파라미터에 대한 미분을 해도 그 값이 0이 나오기 때문에 사실상 파라미터에 대한 최적화 과정이 전부 무의미해진다.
> 
> > ### Max function
> > **0-1 loss function** 에서 발생한 문제, 그러니까 모든 지점에서의 기울기 값이 0인 문제를 해결하기 위해 새롭게 정의한 목적 함수이다.
> > 
> > $\max (0, -(\hat{y}\times y))$
> >
> > 예측 값과 실제 값의 부호가 다르면 $\max (0, +)=+$ 이고,
> > 부호가 다르면 $\max (0, -)=0$ 이라 이렇게 부호가 다른 경우에 한해 파라미터의 최적화가 가능하다.
> >
> > 하지만 **Max function** 또한 실제 값과 예측 값 중 하나라도 0이 나오면 $\max (0, 0)=0$ 이기 때문에 의미있는 추정이었다고 하더라도 그것이 무시가 된다. 이것이 **degenerate solution** 이라는 문제이다.
> 
> > ### Hinge loss function
> > **degenerate solution** 을 해결한, **Max function** 이 개선된 형태의 목적 함수이다.
> >
> > $\max (0, 1-(\hat{y}\times y))$ : *margin* 이라고 하는 1 항 하나가 추가된 별 거 없어보이는 형태이지만, 이 *margin* 을 추가함으로써 **degenerate solution** 문제가 해결된다.
> >
> > **Hinge loss function** 은 **0-1 loss function** 의 *upper bound* : 그 출력이 항상 크거나 같은 포지션이다. **0-1 loss function** 의 출력이 9.8 이면 오차가 발생한 데이터의 개수가 9개라는 것인데, 여기서 **0-1 loss function** 을 **Hinge loss function** 로 그대로 바꿔 넣어도 그 출력은 최소 9.8 이라는 것이다.
> 
> > ### Logistic loss function
> > **0-1 loss function** 의 기본 형태에, `Log - Sum - Exponential` 을 적용해 새로운 형태로 만든 목적 함수이다.
> >
> > $\sum_{i=1}^{n} log(1 + \exp(-(\hat{y}\times y)))$ : 아주 작은 차이를 가진 두 변수에 `Exponential` 을 적용한 결과를 서로 비교해 보면, 그 차이가 매우 커진다. 또한 **Logistic loss function** 의 출력은 모든 지점에서 미분이 가능하기 때문에 목적 함수로서의 가치가 있다.
> >
> > 여기서 발전된 **Sigmoid function** 을 사용해 선형 회귀 모델의 확률적 구분기를 만들 수 있다.

## W9 : 선형 회귀 모델 응용하기
> 이진 분류가 아닌, 다중 클래스로의 분류를 선형 회귀 모델을 통해 수행할 수 있다. 그것의 대표적인 방법 중 **One vs All** : 하나의 클래스와 다른 클래스의 구분을 짓고, 각 클래스 수에 대응하는 만큼의 선형 회귀 모델을 만들어 최종적으로 클래스 간 분류를 진행하는 방법이 있다.
> 
> 

## W10 :선형 회귀 모델의 한계점과 개선 방안
> 선형 회귀 모델은 기본적으로 *outlier* 에 너무나 취약하다. *outlier* 또한 데이터의 일부이니, 선형 회귀 모델은 이 데이터에 모델을 계속 맞추려 할 것이며 이 때문에 오히려 정상적인 범위에 포함된 데이터에 대해선 모델이 잘 맞지 않거나 하는 등의 문제가 발생한다.
>
> > ### Robust Regression
> > **Least Square** 의 경우는 $(예측 값 \- 실제 값)$ 을 제곱한 오차 지표를 사용하기 때문에 그 값이 경우에 따라 매우 커질 가능성이 존재한다. 즉, 줄여야 할 오차 정도가 매우 커진다는 것이다. 
> >
> > 그래서 **Robust Regression** 이라는 방법을 사용한다 : 모델에서 *outlier* 에 대한 비중을 적게 두거나 무시하는 방법, 그리고 오차 정도가 크더라도 줄여야 하는 그것의 크기를 같게 하도록 응용하는 방법이다.
> >
> > 이것의 핵심은 $(예측 값 \- 실제 값)$ 을 제곱하는 것이 아닌 $예측 값 - 실제 값$ 을 그냥 절댓값으로 변환해 결과를 판단하는 것이다.
> 
> > ### Huber loss
> > **Robust Regression** 에서 사용한 절댓값과 **Least Square** 를 함께 사용하는 오차 측정 방식이다. $\epsilon$ 을 기준으로 각 두 방식을 사용하게 된다.
> >
> > $\epsilon$ 은 보통 오차 정도가 낮은 구간에서 **Least Square** 를 사용하도록 설정되고, 오차 정도가 높은 구간에서는 **Robust Regression** 을 사용하도록 설정된다.
> >
> > 이는 모든 구간에서 파라미터의 최적화를 위한 미분을 하기 위해 설정되는 것이다.
> 
> ### RANSAC
> *outlier* 를 무시하고 모델을 만들어 가는 방식이다. *outlier* 의 반대 개념은 *inlier* 인데, 이들 *inlier* 만이 유의미하다고 판단될 때, 이 방법을 사용한다.
>
> > #### 1. 전체 데이터셋 중 일부 데이터만 무작위로 선택한다.
> > #### 2. 선택된 데이터로 초기 상태의 선형 회귀 모델을 구성한다.
> > #### 3. 초기 모델로 기존 데이터셋에 맞추는 과정을 반복해 *inlier*, *outlier* 를 구분한다.
> > #### 4. 가장 많은 *inlier* 를 포함하는 모델을 최종 모델로 선택한다.
> > #### 5. 최종 모델을 전체 데이터셋에 맞춰 본다.

## W11 : 
